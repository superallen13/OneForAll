num_bases: 4
emb_dim: 768
num_layers: 6
dropout: 0.0
JK: "last"
lr: 0.0001
l2: 0
num_epochs: 50
batch_size: 128
num_workers: 4
seed: 1
data_path:
offline_log: True
exp_name: ""
train_sample_size: -1
eval_sample_size: -1
rwpe:
task_names:
  - arxiv
llm_name: "llama2_7b"
llm_b_size: 1
load_texts: False
max_nodes_per_hop: 100
load_best: False
test_rep: 1
val_interval: 1